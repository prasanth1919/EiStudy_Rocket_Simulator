The project titled "Sentiment Analysis in Low-Resource Languages using LLMs" addresses the challenge of performing accurate sentiment analysis for languages with limited labeled data and linguistic resources. While sentiment analysis has seen significant progress in resource-rich languages like English, low-resource languages remain underserved due to a lack of data and tools. This project explores the potential of Large Language Models (LLMs) such as mBERT, XLM-R, and others to overcome these limitations by leveraging their ability to generalize from small datasets. The goal is to fine-tune these LLMs on multilingual data to improve sentiment detection in less common languages, thereby bridging the gap in sentiment analysis for underrepresented communities. By focusing on models that balance accuracy and F1-score, the project aims to establish new benchmarks for inclusive NLP, providing tools that are accessible to speakers of low-resource languages. This research also explores novel approaches like adapter-based fine-tuning to enhance performance on sparse and noisy data, making significant contributions to the field of NLP for low-resource languages.
